{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Burgers' Equation Using PINNs\n",
    "\n",
    "### Problem Statement\n",
    "We aim to solve the **Burgers' equation** using **Physics-Informed Neural Networks (PINNs)**. ### Context: What is Burgers' Equation and Why is it Useful?\n",
    "\n",
    "The **Burgers' equation** is a fundamental partial differential equation (PDE) that appears in various fields of applied mathematics, physics, and engineering. It is expressed as:\n",
    "\n",
    "$$\n",
    "u_t + u u_x = \\nu u_{xx},\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $u(t, x)$ is the unknown function,\n",
    "- $u_t$ represents the temporal derivative,\n",
    "- $u_x$ is the spatial derivative,\n",
    "- $u_{xx}$ is the second spatial derivative,\n",
    "- $\\nu$ is the viscosity term (a positive constant).\n",
    "\n",
    "Burgers' equation is particularly useful because it models a wide range of physical phenomena, including **nonlinear wave propagation**, **shock waves**, **turbulence**, and **heat conduction**. It combines two key effects:\n",
    "1. **Nonlinear advection** (through the $ u u_x $ term), which governs the steepening of waves.\n",
    "2. **Linear diffusion** (through the $ \\nu u_{xx} $ term), which smoothens sharp gradients.\n",
    "\n",
    "This combination of effects makes Burgers' equation an important test case for studying the interplay between nonlinearity and diffusion in complex systems.\n",
    "\n",
    "In practice, Burgers' equation is often used as a benchmark problem to validate new **numerical methods**, **machine learning techniques**, and **data-driven approaches**. Known analytical solutions for specific cases provide a reference for comparing approximate solutions, such as those computed using **Physics-Informed Neural Networks (PINNs)**. Solving Burgers' equation helps researchers explore broader phenomena in **fluid dynamics**, **traffic flow**, and **acoustic waves**, while also assessing the effectiveness of computational techniques.\n",
    "\n",
    "The initial and boundary conditions are:\n",
    "\n",
    "- **Initial Condition**:\n",
    "  $$\n",
    "  u(0, x) = -\\sin(\\pi x), \\quad x \\in [-1, 1].\n",
    "  $$\n",
    "- **Boundary Conditions**:\n",
    "  $$\n",
    "  u(t, -1) = u(t, 1) = 0, \\quad t \\in [0, 1].\n",
    "  $$\n",
    "\n",
    "The goal is to approximate $u(t, x)$ using a PINN over the domain $t \\in [0, 1], x \\in [-1, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "PINNs use a neural network to represent the solution \\( u(t, x) \\). The network takes the inputs \\( t \\) and \\( x \\) and outputs \\( u(t, x) \\). Here's the structure:\n",
    "- Input layer: 2 features (\\( t \\) and \\( x \\))\n",
    "- Hidden layers: 4 fully connected layers with 50 neurons each\n",
    "- Activation function: `tanh` for all hidden layers\n",
    "- Output layer: A single scalar representing \\( u(t, x) \\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(PINN, self).__init__()\n",
    "        self.input_layer = nn.Linear(2, layers[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.hidden_layers.append(nn.Linear(layers[i], layers[i + 1]))\n",
    "        self.output_layer = nn.Linear(layers[-1], 1)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        input_tensor = torch.cat([t, x], dim=1)\n",
    "        input_tensor = self.activation(self.input_layer(input_tensor))\n",
    "        for layer in self.hidden_layers:\n",
    "            input_tensor = self.activation(layer(input_tensor))\n",
    "        return self.output_layer(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physics-Informed Loss Function\n",
    "The PINN minimizes two loss terms:\n",
    "1. **Data Loss**:\n",
    "   ```math\n",
    "   \\text{Loss}_u = \\frac{1}{N_u} \\sum_{i=1}^{N_u} \\left( u_i - u_\\text{pred}(t_i, x_i) \\right)^2\n",
    "   ```\n",
    "2. **PDE Residual Loss**:\n",
    "   ```math\n",
    "   \\text{Loss}_f = \\frac{1}{N_f} \\sum_{i=1}^{N_f} f(t_i, x_i)^2,\n",
    "   ```\n",
    "   where:\n",
    "   ```math\n",
    "   f(t, x) = u_t + u u_x - \\nu u_{xx}.\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the physics-informed loss function\n",
    "def pde_loss(model, t, x):\n",
    "    t.requires_grad_(True)\n",
    "    x.requires_grad_(True)\n",
    "\n",
    "    u = model(t, x)\n",
    "\n",
    "    # Compute partial derivatives\n",
    "    u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n",
    "\n",
    "    # Define the PDE residual\n",
    "    f = u_t + u * u_x - (0.01 / np.pi) * u_xx\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Data\n",
    "To train the PINN, we need:\n",
    "- Initial condition data: \\( u(0, x) = -\\sin(\\pi x) \\)\n",
    "- Boundary condition data: \\( u(t, -1) = u(t, 1) = 0 \\)\n",
    "- Collocation points: Randomly sample \\( t \\in [0, 1] \\) and \\( x \\in [-1, 1] \\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data():\n",
    "    # Initial condition\n",
    "    x = np.linspace(-1, 1, 256).reshape(-1, 1)\n",
    "    t = np.zeros_like(x)\n",
    "    u = -np.sin(np.pi * x)\n",
    "\n",
    "    # Boundary conditions\n",
    "    t_boundary = np.linspace(0, 1, 100).reshape(-1, 1)\n",
    "    x_left = -np.ones_like(t_boundary)\n",
    "    x_right = np.ones_like(t_boundary)\n",
    "    u_left = np.zeros_like(t_boundary)\n",
    "    u_right = np.zeros_like(t_boundary)\n",
    "\n",
    "    # Collocation points\n",
    "    t_f = np.random.uniform(0, 1, 10000).reshape(-1, 1)\n",
    "    x_f = np.random.uniform(-1, 1, 10000).reshape(-1, 1)\n",
    "\n",
    "    # Convert all to PyTorch tensors\n",
    "    t_u = torch.tensor(t, dtype=torch.float32)\n",
    "    x_u = torch.tensor(x, dtype=torch.float32)\n",
    "    u = torch.tensor(u, dtype=torch.float32)\n",
    "    t_boundary = torch.tensor(t_boundary, dtype=torch.float32)\n",
    "    x_left = torch.tensor(x_left, dtype=torch.float32)\n",
    "    x_right = torch.tensor(x_right, dtype=torch.float32)\n",
    "    u_left = torch.tensor(u_left, dtype=torch.float32)\n",
    "    u_right = torch.tensor(u_right, dtype=torch.float32)\n",
    "    t_f = torch.tensor(t_f, dtype=torch.float32)\n",
    "    x_f = torch.tensor(x_f, dtype=torch.float32)\n",
    "\n",
    "    return t_u, x_u, u, t_boundary, x_left, x_right, u_left, u_right, t_f, x_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the PINN\n",
    "We combine the data loss and PDE residual loss to train the PINN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training loop\n",
    "def train(model, optimizer, t_u, x_u, u, t_f, x_f, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Data loss\n",
    "        u_pred = model(t_u, x_u)\n",
    "        loss_u = torch.mean((u - u_pred) ** 2)\n",
    "\n",
    "        # PDE residual loss\n",
    "        f_pred = pde_loss(model, t_f, x_f)\n",
    "        loss_f = torch.mean(f_pred ** 2)\n",
    "\n",
    "        # Total loss\n",
    "        loss = loss_u + loss_f\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}, Loss_u: {loss_u.item()}, Loss_f: {loss_f.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "After training, we plot the predicted solution and the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting function\n",
    "def plot_results(model):\n",
    "    t = np.linspace(0, 1, 100)\n",
    "    x = np.linspace(-1, 1, 256)\n",
    "    T, X = np.meshgrid(t, x)\n",
    "\n",
    "    t_test = torch.tensor(T.flatten().reshape(-1, 1), dtype=torch.float32)\n",
    "    x_test = torch.tensor(X.flatten().reshape(-1, 1), dtype=torch.float32)\n",
    "    u_pred = model(t_test, x_test).detach().numpy().reshape(256, 100)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(u_pred, extent=[0, 1, -1, 1], origin='lower', aspect='auto', cmap='jet')\n",
    "    plt.colorbar(label='u(t, x)')\n",
    "    plt.xlabel('t')\n",
    "    plt.ylabel('x')\n",
    "    plt.title('Predicted solution u(t, x)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.5301456451416016, Loss_u: 0.52680504322052, Loss_f: 0.0033406266011297703\n",
      "Epoch 100, Loss: 0.14273175597190857, Loss_u: 0.0984220877289772, Loss_f: 0.04430966079235077\n",
      "Epoch 200, Loss: 0.11678905785083771, Loss_u: 0.07471741735935211, Loss_f: 0.042071644216775894\n",
      "Epoch 300, Loss: 0.09481151401996613, Loss_u: 0.060708366334438324, Loss_f: 0.0341031476855278\n",
      "Epoch 400, Loss: 0.0851333737373352, Loss_u: 0.052189841866493225, Loss_f: 0.03294353559613228\n",
      "Epoch 500, Loss: 0.0776607096195221, Loss_u: 0.04729670286178589, Loss_f: 0.030364006757736206\n",
      "Epoch 600, Loss: 0.0673774927854538, Loss_u: 0.04164574667811394, Loss_f: 0.025731749832630157\n",
      "Epoch 700, Loss: 0.05717775970697403, Loss_u: 0.034085530787706375, Loss_f: 0.023092228919267654\n",
      "Epoch 800, Loss: 0.05265479534864426, Loss_u: 0.027640676125884056, Loss_f: 0.02501412108540535\n",
      "Epoch 900, Loss: 0.06280390918254852, Loss_u: 0.023775428533554077, Loss_f: 0.039028480648994446\n",
      "Epoch 1000, Loss: 0.02946292981505394, Loss_u: 0.017172586172819138, Loss_f: 0.012290344573557377\n",
      "Epoch 1100, Loss: 0.0236184261739254, Loss_u: 0.013634097762405872, Loss_f: 0.009984329342842102\n",
      "Epoch 1200, Loss: 0.02020205557346344, Loss_u: 0.010991874150931835, Loss_f: 0.00921018049120903\n",
      "Epoch 1300, Loss: 0.015321859158575535, Loss_u: 0.008461518213152885, Loss_f: 0.006860340945422649\n",
      "Epoch 1400, Loss: 0.014498746022582054, Loss_u: 0.00760206114500761, Loss_f: 0.006896684877574444\n",
      "Epoch 1500, Loss: 0.011435266584157944, Loss_u: 0.006220259703695774, Loss_f: 0.005215007346123457\n",
      "Epoch 1600, Loss: 0.009898534044623375, Loss_u: 0.0052411966025829315, Loss_f: 0.004657337907701731\n",
      "Epoch 1700, Loss: 0.009295969270169735, Loss_u: 0.005111896898597479, Loss_f: 0.004184072371572256\n",
      "Epoch 1800, Loss: 0.008198663592338562, Loss_u: 0.004308870527893305, Loss_f: 0.00388979259878397\n",
      "Epoch 1900, Loss: 0.007388212252408266, Loss_u: 0.0037647606804966927, Loss_f: 0.0036234515719115734\n",
      "Epoch 2000, Loss: 0.007004675921052694, Loss_u: 0.0036608451046049595, Loss_f: 0.003343830816447735\n",
      "Epoch 2100, Loss: 0.0064181676134467125, Loss_u: 0.003236171090975404, Loss_f: 0.003181996289640665\n",
      "Epoch 2200, Loss: 0.011367984116077423, Loss_u: 0.0034354720264673233, Loss_f: 0.0079325120896101\n",
      "Epoch 2300, Loss: 0.00569148687645793, Loss_u: 0.002893783152103424, Loss_f: 0.0027977037243545055\n",
      "Epoch 2400, Loss: 0.005269412882626057, Loss_u: 0.0026168683543801308, Loss_f: 0.002652544528245926\n",
      "Epoch 2500, Loss: 0.006641061045229435, Loss_u: 0.002700445242226124, Loss_f: 0.003940615803003311\n",
      "Epoch 2600, Loss: 0.0047000134363770485, Loss_u: 0.002376141492277384, Loss_f: 0.0023238721769303083\n",
      "Epoch 2700, Loss: 0.00435410812497139, Loss_u: 0.002157222479581833, Loss_f: 0.0021968854125589132\n",
      "Epoch 2800, Loss: 0.005894389934837818, Loss_u: 0.0022539361380040646, Loss_f: 0.0036404537968337536\n",
      "Epoch 2900, Loss: 0.003907325211912394, Loss_u: 0.001979663036763668, Loss_f: 0.0019276622915640473\n",
      "Epoch 3000, Loss: 0.003616608679294586, Loss_u: 0.0018013219814747572, Loss_f: 0.0018152868142351508\n",
      "Epoch 3100, Loss: 0.009248224087059498, Loss_u: 0.0019320582505315542, Loss_f: 0.007316166069358587\n",
      "Epoch 3200, Loss: 0.003238022793084383, Loss_u: 0.001650970778428018, Loss_f: 0.001587052014656365\n",
      "Epoch 3300, Loss: 0.002987504471093416, Loss_u: 0.0015039652353152633, Loss_f: 0.001483539235778153\n",
      "Epoch 3400, Loss: 0.004696283023804426, Loss_u: 0.0015555412974208593, Loss_f: 0.003140741726383567\n",
      "Epoch 3500, Loss: 0.0027088825590908527, Loss_u: 0.0014123955043032765, Loss_f: 0.0012964869383722544\n",
      "Epoch 3600, Loss: 0.002511397935450077, Loss_u: 0.0012907346244901419, Loss_f: 0.0012206633109599352\n",
      "Epoch 3700, Loss: 0.003953444771468639, Loss_u: 0.0013206624425947666, Loss_f: 0.0026327823288738728\n",
      "Epoch 3800, Loss: 0.0022549089044332504, Loss_u: 0.0011774043086916208, Loss_f: 0.0010775045957416296\n",
      "Epoch 3900, Loss: 0.00248679518699646, Loss_u: 0.001154344412498176, Loss_f: 0.0013324507744982839\n",
      "Epoch 4000, Loss: 0.0023623511660844088, Loss_u: 0.0011035510106012225, Loss_f: 0.0012588001554831862\n",
      "Epoch 4100, Loss: 0.002909841248765588, Loss_u: 0.001045209588482976, Loss_f: 0.0018646316602826118\n",
      "Epoch 4200, Loss: 0.0020883549004793167, Loss_u: 0.0009955251589417458, Loss_f: 0.001092829741537571\n",
      "Epoch 4300, Loss: 0.0017658830620348454, Loss_u: 0.0009301988175138831, Loss_f: 0.0008356843027286232\n",
      "Epoch 4400, Loss: 0.002161733340471983, Loss_u: 0.0008705461514182389, Loss_f: 0.001291187247261405\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    layers = [50, 50, 50, 50]\n",
    "    model = PINN(layers)\n",
    "\n",
    "    t_u, x_u, u, t_boundary, x_left, x_right, u_left, u_right, t_f, x_f = create_training_data()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train(model, optimizer, t_u, x_u, u, t_f, x_f, epochs=5000)\n",
    "    plot_results(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
